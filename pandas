
Creating an automated reconciliation process for Excel workbooks against SQL outputs and Excel formulas is a complex task. Here's a high-level approach, followed by a detailed example:

Approach:
Central Repository for Formulas: Maintain a central repository (could be a database or another Excel workbook) that holds the metric names, their corresponding SQL or Excel formulas, and the workbook they belong to.

Excel Reader: Write code to read data from the Excel workbook.

SQL Executor: Write code to execute SQL against your database and get results.

Formula Evaluator: Write code to evaluate Excel formulas.

Reconciliation: Compare the value from the Excel workbook against the calculated value (either SQL or Excel formula). If there's a difference, report it.

Reporting: Log the differences and associated metric/formula for review.

Detailed Example:
For simplicity, let's assume we're using Python with the pandas library for data manipulation, openpyxl for Excel interaction, and sqlite3 for SQL execution:

Central Repository:
Suppose you maintain a central Excel workbook central_repo.xlsx:

sql
Copy code
| MetricName | FormulaType | Formula                           | WorkbookName |
|------------|-------------|-----------------------------------|--------------|
| Metric1    | SQL         | SELECT value FROM table WHERE ... | workbook1.xlsx |
| Metric2    | Excel       | A1+B2                             | workbook1.xlsx |
... (and so on)
Excel Reader:
python
Copy code
import pandas as pd

def read_excel_data(workbook_name, column_name):
    df = pd.read_excel(workbook_name)
    return df[column_name].values
SQL Executor:
python
Copy code
import sqlite3

def execute_sql(sql_query):
    conn = sqlite3.connect('your_database_name.db')
    cur = conn.cursor()
    cur.execute(sql_query)
    result = cur.fetchone()[0]
    conn.close()
    return result
Formula Evaluator:
python
Copy code
def evaluate_excel_formula(formula, df):
    return df.eval(formula)
Reconciliation:
python
Copy code
def reconcile():
    repo_df = pd.read_excel('central_repo.xlsx')
    differences = []

    for index, row in repo_df.iterrows():
        workbook_value = read_excel_data(row['WorkbookName'], row['MetricName'])
        
        if row['FormulaType'] == 'SQL':
            calculated_value = execute_sql(row['Formula'])
        else:
            calculated_value = evaluate_excel_formula(row['Formula'], workbook_value)

        if workbook_value != calculated_value:
            differences.append({
                'Metric': row['MetricName'],
                'Workbook': row['WorkbookName'],
                'WorkbookValue': workbook_value,
                'CalculatedValue': calculated_value,
                'Formula': row['Formula']
            })

    return differences
Reporting:
python
Copy code
def report_differences():
    diffs = reconcile()
    for diff in diffs:
        print(f"Metric: {diff['Metric']} in {diff['Workbook']} has a value of {diff['WorkbookValue']}, but the calculated value is {diff['CalculatedValue']} using formula: {diff['Formula']}")
Execution:
python
Copy code
if __name__ == "__main__":
    report_differences()
Note:

This is a simplified example and assumes many things for the sake of clarity. You might need to expand upon this, handle errors, consider performance implications, etc.
It assumes that the SQL results return a single value. Adjustments are needed for more complex SQL results.
Ensure your Excel workbooks, the central repo, and the database are all accessible and correctly referenced in the code.
Use the appropriate database library if not using SQLite.
Lastly, for taking input on the metric logic of SQL or the Excel formula, you could either update the central_repo.xlsx manually or create a UI to input the details seamlessly.





Prerequisites:
Install necessary libraries:

bash
Copy code
pip install pandas openpyxl teradata
Central Repository:
Still assuming you maintain a central Excel workbook central_repo.xlsx as described before.

Code:
python
Copy code
import pandas as pd
import teradata

# Database credentials
udaExec = teradata.UdaExec(appName="ExcelReconciliation", version="1.0", logConsole=False)

# Connect to Teradata
def connect_to_teradata():
    return udaExec.connect(method="odbc", system="your_teradata_system",
                           username="your_username", password="your_password");

# Read data from Excel workbook
def read_excel_data(workbook_name, column_name):
    df = pd.read_excel(workbook_name)
    return df[column_name].values

# Execute SQL against Teradata and return result
def execute_sql(sql_query):
    with connect_to_teradata() as session:
        result = session.execute(sql_query).fetchone()
        return result[0]

# Evaluate Excel formula
def evaluate_excel_formula(formula, df):
    return df.eval(formula)

# Reconciliation function
def reconcile():
    repo_df = pd.read_excel('central_repo.xlsx')
    differences = []

    for index, row in repo_df.iterrows():
        workbook_value = read_excel_data(row['WorkbookName'], row['MetricName'])
        
        if row['FormulaType'] == 'SQL':
            calculated_value = execute_sql(row['Formula'])
        else:
            calculated_value = evaluate_excel_formula(row['Formula'], workbook_value)

        if workbook_value != calculated_value:
            differences.append({
                'Metric': row['MetricName'],
                'Workbook': row['WorkbookName'],
                'WorkbookValue': workbook_value,
                'CalculatedValue': calculated_value,
                'Formula': row['Formula']
            })

    return differences

# Report differences
def report_differences():
    diffs = reconcile()
    for diff in diffs:
        print(f"Metric: {diff['Metric']} in {diff['Workbook']} has a value of {diff['WorkbookValue']}, but the calculated value is {diff['CalculatedValue']} using formula: {diff['Formula']}")

# Execution
if __name__ == "__main__":
    report_differences()
Note:

You'll need to replace placeholders like your_teradata_system, your_username, and your_password with your actual database details.
This is a basic example and will need modifications based on real-world complexities like handling multiple SQL rows, Excel formula specifics, etc.
Ensure that your Teradata ODBC driver is installed and properly configured on the machine running the script.
Always use sensitive data like passwords with caution. Ideally, use more secure methods to handle them rather than hardcoding directly into scripts.
Handle exceptions, edge cases, and potential errors in the code for a robust solution. This is a simplistic approach for demonstration.




I want to do some other formatting as well like creating new column in excel and copying formatting styles font etc from other columns 

To manipulate Excel workbooks, including copying formatting and adding columns, the openpyxl library in Python is a fantastic resource.

Let me guide you through how you could do this:

1. Prerequisites:
Ensure you've installed the necessary libraries:

bash
Copy code
pip install openpyxl pandas
2. Manipulating Excel Files:
Here's how you can create a new column and copy formatting from an existing column:

python
Copy code
import openpyxl
from openpyxl.styles import Font, PatternFill, Border

def copy_formatting(src_cell, dst_cell):
    if src_cell.has_style:
        dst_cell.font = src_cell.font.copy()
        dst_cell.border = src_cell.border.copy()
        dst_cell.fill = src_cell.fill.copy()
        dst_cell.number_format = src_cell.number_format
        dst_cell.protection = src_cell.protection.copy()
        dst_cell.alignment = src_cell.alignment.copy()

def add_column_with_formatting(workbook_path, sheet_name, new_col_idx, copy_format_from_col):
    # Load the workbook and select the sheet
    wb = openpyxl.load_workbook(workbook_path)
    sheet = wb[sheet_name]
    
    # Insert a new column
    sheet.insert_cols(new_col_idx)
    
    for row in sheet.iter_rows(min_col=new_col_idx, max_col=new_col_idx):
        for cell in row:
            reference_cell = sheet.cell(row=cell.row, column=copy_format_from_col)
            copy_formatting(reference_cell, cell)
    
    # Save the changes
    wb.save(workbook_path)

# Example usage:
# This will add a new column at index 3 (C) and copy the formatting from column 2 (B).
add_column_with_formatting('your_workbook.xlsx', 'Sheet1', 3, 2)
Explanation:
The copy_formatting function takes a source cell and a destination cell as arguments. It copies the style attributes from the source cell to the destination cell.

The add_column_with_formatting function takes the workbook's path, sheet's name, the index for the new column, and the index of the column from which formatting will be copied. It inserts a new column and then goes through each cell in the new column to apply the copied formatting.

Remember, while this will cover many common use cases for formatting, Excel has a wide variety of formatting options. This script might not copy every single nuanced detail but should handle most basic formatting scenarios.

Adjustments can be made for more specific requirements or to handle advanced formatting cases.





Below is a comprehensive solution that combines:

Reading from a central repository (central_repo.xlsx).
Reconciliation of metrics between SQL results and Excel values.
Adjusting an Excel workbook by adding a new column and copying formatting from an existing column.
Prerequisites:
Ensure you've installed the necessary libraries:

bash
Copy code
pip install openpyxl pandas teradata
Complete Code:
python
Copy code
import pandas as pd
import teradata
import openpyxl
from openpyxl.styles import Font, PatternFill, Border

# Database setup
udaExec = teradata.UdaExec(appName="ExcelReconciliation", version="1.0", logConsole=False)

def connect_to_teradata():
    return udaExec.connect(method="odbc", system="your_teradata_system",
                           username="your_username", password="your_password")

# Read data from Excel workbook
def read_excel_data(workbook_name, column_name):
    df = pd.read_excel(workbook_name)
    return df[column_name].values

# Execute SQL against Teradata and return result
def execute_sql(sql_query):
    with connect_to_teradata() as session:
        result = session.execute(sql_query).fetchone()
        return result[0]

# Evaluate Excel formula
def evaluate_excel_formula(formula, df):
    return df.eval(formula)

# Function to copy formatting from one cell to another
def copy_formatting(src_cell, dst_cell):
    if src_cell.has_style:
        dst_cell.font = src_cell.font.copy()
        dst_cell.border = src_cell.border.copy()
        dst_cell.fill = src_cell.fill.copy()
        dst_cell.number_format = src_cell.number_format
        dst_cell.protection = src_cell.protection.copy()
        dst_cell.alignment = src_cell.alignment.copy()

# Function to add a new column and copy formatting from an existing column
def add_column_with_formatting(workbook_path, sheet_name, new_col_idx, copy_format_from_col):
    wb = openpyxl.load_workbook(workbook_path)
    sheet = wb[sheet_name]
    sheet.insert_cols(new_col_idx)
    
    for row in sheet.iter_rows(min_col=new_col_idx, max_col=new_col_idx):
        for cell in row:
            reference_cell = sheet.cell(row=cell.row, column=copy_format_from_col)
            copy_formatting(reference_cell, cell)
    
    wb.save(workbook_path)

# Reconciliation function
def reconcile():
    repo_df = pd.read_excel('central_repo.xlsx')
    differences = []

    for index, row in repo_df.iterrows():
        workbook_value = read_excel_data(row['WorkbookName'], row['MetricName'])
        
        if row['FormulaType'] == 'SQL':
            calculated_value = execute_sql(row['Formula'])
        else:
            calculated_value = evaluate_excel_formula(row['Formula'], workbook_value)

        if workbook_value != calculated_value:
            differences.append({
                'Metric': row['MetricName'],
                'Workbook': row['WorkbookName'],
                'WorkbookValue': workbook_value,
                'CalculatedValue': calculated_value,
                'Formula': row['Formula']
            })

    return differences

# Reporting function
def report_differences():
    diffs = reconcile()
    for diff in diffs:
        print(f"Metric: {diff['Metric']} in {diff['Workbook']} has a value of {diff['WorkbookValue']}, but the calculated value is {diff['CalculatedValue']} using formula: {diff['Formula']}")

    # Add a new column and copy formatting for demonstration purposes.
    # For instance, this adds a new column at index 3 (C) and copies formatting from column 2 (B) in all flagged workbooks.
    for workbook in set([diff['Workbook'] for diff in diffs]):
        add_column_with_formatting(workbook, 'Sheet1', 3, 2)
        print(f"Added a new column with copied formatting in {workbook}")

# Execution
if __name__ == "__main__":
    report_differences()
Note:

You need to replace placeholders like your_teradata_system, your_username, and your_password with your actual Teradata database details.
This code assumes that every time there's a discrepancy between the SQL and Excel values, you want to add a new column to the relevant workbook and copy the formatting from an existing column.
Make sure you backup your Excel files before running the script to avoid unintentional changes.
Always use sensitive data like passwords with caution. Ideally, use more secure methods to handle them rather than hardcoding directly into scripts.




Making SQL dynamic based on Excel filters introduces additional complexity. However, it's achievable with openpyxl which can read filter settings.

Here's an approach:

Read Filters in Excel: For each column in the Excel workbook with an active filter, determine the filter conditions/values.
Dynamic SQL Creation: Based on the filter conditions/values, dynamically modify your SQL to include WHERE clauses or equivalent.
Reconciliation: Run the dynamically generated SQL and perform reconciliation.
I'll show you how to incorporate these steps into the previous solution.

Step 1: Read Filters in Excel
First, let's create a function to read the filter conditions from an Excel sheet:

python
Copy code
from openpyxl.worksheet.filters import FilterCell

def get_excel_filters(workbook_path, sheet_name):
    wb = openpyxl.load_workbook(workbook_path)
    sheet = wb[sheet_name]
    filters = {}
    
    auto_filter = sheet.auto_filter
    if not auto_filter:
        return None

    for column_letter, column_filter in auto_filter.columns.items():
        conditions = [cell.val for cell in column_filter.filters if isinstance(cell, FilterCell)]
        if conditions:
            filters[column_letter] = conditions

    return filters
This function returns a dictionary where keys are column letters with active filters, and values are lists of filter conditions for the respective columns.

Step 2: Dynamic SQL Creation
Modify the execute_sql function to dynamically build SQL based on filters:

python
Copy code
def execute_sql(sql_query, filters=None):
    if filters:
        where_clauses = []
        for col, conditions in filters.items():
            # Assuming mapping between Excel columns and database columns (modify as needed)
            db_col = excel_to_db_mapping.get(col)  
            clause = f"{db_col} IN ({', '.join(map(str, conditions))})"
            where_clauses.append(clause)

        if where_clauses:
            sql_query += " WHERE " + " AND ".join(where_clauses)

    with connect_to_teradata() as session:
        result = session.execute(sql_query).fetchone()
        return result[0]
For this to work, you'll need a mapping between Excel column letters and corresponding database column names. For demonstration purposes, let's assume:

python
Copy code
excel_to_db_mapping = {
    'A': 'db_col1',
    'B': 'db_col2',
    # ... add more mappings as needed
}
Step 3: Reconciliation
Modify the reconciliation loop to consider filters:

python
Copy code
for index, row in repo_df.iterrows():
    workbook_value = read_excel_data(row['WorkbookName'], row['MetricName'])
    
    # Get Excel filters
    filters = get_excel_filters(row['WorkbookName'], 'Sheet1')
    
    if row['FormulaType'] == 'SQL':
        calculated_value = execute_sql(row['Formula'], filters)
    else:
        calculated_value = evaluate_excel_formula(row['Formula'], workbook_value)
    # ... rest of the loop remains the same
This solution assumes that each SQL statement only pertains to a single Excel file. If an SQL statement is designed to aggregate or compare across multiple Excel files, you'll need a more intricate solution that can dynamically build and adjust SQL based on multiple filter sets.

Lastly, dynamic SQL can be a source of SQL injection if not handled correctly. Always ensure that dynamic values being inserted into SQL statements are strictly controlled or sanitized to prevent any security vulnerabilities.





Lets say we would provide the column name and values selected in filter by a variable or something in the code or as configuration read from
How will the code handle that

If you're providing the filtered column names and values as a configuration, there are multiple ways you could structure this:

Configuration File: You could maintain a configuration file (e.g., JSON, YAML, etc.) for each workbook where you specify the filters.
Inline Variables: Directly in the code, you could declare a dictionary for each workbook with the filters.
For simplicity, I'll demonstrate the inline variable approach, but the idea would be the same if reading from a file.

Example:
Specify Filters in a Dictionary:
python
Copy code
filters_config = {
    "workbook1.xlsx": {
        "ColumnName1": ["value1", "value2"],
        "ColumnName2": ["valueA"]
    },
    "workbook2.xlsx": {
        "ColumnNameX": ["valueY"]
    },
    # ... other workbooks
}
Modify the SQL Execution to Consider the Filters:
python
Copy code
def execute_sql(workbook_name, sql_query):
    filters = filters_config.get(workbook_name, {})
    
    where_clauses = []
    for col, conditions in filters.items():
        db_col = excel_to_db_mapping.get(col)  # Mapping between Excel columns and database columns
        clause = f"{db_col} IN ({', '.join(map(lambda x: f'\'{x}\'', conditions))})"
        where_clauses.append(clause)

    if where_clauses:
        sql_query += " WHERE " + " AND ".join(where_clauses)

    with connect_to_teradata() as session:
        result = session.execute(sql_query).fetchone()
        return result[0]
Reconciliation Loop:
During the reconciliation, you'll just call the modified execute_sql function:

python
Copy code
for index, row in repo_df.iterrows():
    workbook_value = read_excel_data(row['WorkbookName'], row['MetricName'])
    
    if row['FormulaType'] == 'SQL':
        calculated_value = execute_sql(row['WorkbookName'], row['Formula'])
    else:
        calculated_value = evaluate_excel_formula(row['Formula'], workbook_value)
    # ... rest of the loop remains the same
By this method, you can easily update the filters_config dictionary to change the filter conditions/values for any workbook. If using a configuration file, you'd read this configuration into the filters_config dictionary at the start of your script, but the rest would work the same.





OK lets say the excel workbook has sheet named with dashboardname like dashboard1 and its has column A with metric name and the column B has SQL or formula referencing the metric names like metricA- metric B = metric K, this is the excel repository with logic. This workbook has other dashboard data as well.

the other excel workbook has the sheet with same name as repository workbook sheet and it has the same metric in column A and in other columns they have values for different month of the year and quarter or YTD values

I would think if we can read the excel repository and read the metric name and its logic like SQL or formula as dictionary and then resolve the SQL and formula value and create new dictionary

next the other excel sheet has metric name and we would ask for a month column to get the values from so we will have another dictionary 

now we would want to match both the dictionary by metric name and compare the value from SQL or formula to the value in other excel and report it out as another column called Recon (Add the month column name to this oclumn name) and give the result

Now the logic we would want should take the dashboardname and find the sheet on both excel workbook and perform reconcilition and report out

Let me know if you have any questions or need more detail to answer this to the best

Based on your requirements, we can break down the task as follows:

Repository Workbook Parsing: Read the dashboard sheet from the repository workbook. Parse metrics and their respective SQL or Excel formulas into a dictionary.

Metrics Workbook Parsing: Read the dashboard sheet from the metrics workbook. Parse metrics and the given month column into another dictionary.

Reconciliation: Resolve values from the SQL or Excel formulas, then match these values to the values from the metrics workbook. Create a new dictionary containing the results.

Output to Excel: Write the reconciliation results into the metrics workbook in a new column titled "Recon {Month}".

Here's the code that performs this:

python
Copy code
import pandas as pd
import teradata
import openpyxl

# Connect to Teradata database
udaExec = teradata.UdaExec(appName="ExcelReconciliation", version="1.0", logConsole=False)

def connect_to_teradata():
    return udaExec.connect(method="odbc", system="your_teradata_system",
                           username="your_username", password="your_password")

# 1. Repository Workbook Parsing
def get_repository_data(dashboard_name, repo_workbook_path):
    df = pd.read_excel(repo_workbook_path, sheet_name=dashboard_name)
    return dict(zip(df['MetricName'], df['Formula']))

# 2. Metrics Workbook Parsing
def get_metrics_data(dashboard_name, metrics_workbook_path, month_column):
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name)
    return dict(zip(df['MetricName'], df[month_column]))

# Execute SQL against Teradata
def execute_sql(sql_query):
    with connect_to_teradata() as session:
        result = session.execute(sql_query).fetchone()
        return result[0]

# 3. Reconciliation
def perform_reconciliation(dashboard_name, repo_workbook_path, metrics_workbook_path, month_column):
    repo_data = get_repository_data(dashboard_name, repo_workbook_path)
    metrics_data = get_metrics_data(dashboard_name, metrics_workbook_path, month_column)
    
    reconciliation_results = {}
    for metric, formula in repo_data.items():
        if "SELECT" in formula:  # Assuming it's an SQL query
            calculated_value = execute_sql(formula)
        else:  # Assuming it's an Excel formula
            # Replacing metric names with their values from metrics_data
            for m, v in metrics_data.items():
                formula = formula.replace(m, str(v))
            calculated_value = eval(formula)
        
        actual_value = metrics_data.get(metric)
        status = "Match" if actual_value == calculated_value else "Mismatch"
        reconciliation_results[metric] = status

    return reconciliation_results

# 4. Output to Excel
def write_results_to_excel(dashboard_name, metrics_workbook_path, month_column, reconciliation_results):
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name)
    df[f"Recon {month_column}"] = df['MetricName'].map(reconciliation_results)
    df.to_excel(metrics_workbook_path, sheet_name=dashboard_name, index=False)

# Main Function
def main_reconciliation(dashboard_name, repo_workbook_path, metrics_workbook_path, month_column):
    results = perform_reconciliation(dashboard_name, repo_workbook_path, metrics_workbook_path, month_column)
    write_results_to_excel(dashboard_name, metrics_workbook_path, month_column, results)

# Execute
dashboard_name = "dashboard1"
repo_workbook_path = "path_to_repository_workbook.xlsx"
metrics_workbook_path = "path_to_metrics_workbook.xlsx"
month_column = "January"  # Just as an example

main_reconciliation(dashboard_name, repo_workbook_path, metrics_workbook_path, month_column)
Ensure you fill in your Teradata connection details in the connect_to_teradata() function.

Here's a brief explanation:

The get_repository_data() function reads the repository workbook for the given dashboard name and returns a dictionary of metrics and their respective formulas.

The get_metrics_data() function reads the metrics workbook for the given dashboard name and month and returns a dictionary of metrics and their month values.

The perform_reconciliation() function goes through each metric from the repository, calculates its value based on whether it's an SQL or Excel formula, then matches it with the metric value from the metrics workbook.

The write_results_to_excel() function adds a new column "Recon {Month}" to the metrics workbook with the reconciliation results for each metric.

This solution assumes:

Formulas in the repository workbook either contain SQL queries or Excel-like formulas referencing other metrics.

The metrics workbook only contains raw values, and these values are used in the formulas from the repository workbook.

Lastly, take backups before running the script to ensure you don't unintentionally modify essential data.





So lets say we take this example
dashboard_name = "dashboard1"
repo_workbook_path = "path_to_repository_workbook.xlsx"
metrics_workbook_path = "path_to_metrics_workbook.xlsx"
month_column = "January"  # Just as an example

for month_column = January or Jan23 how does the code find out which column has column name January or Jan23

To determine which column in the Excel sheet has a specific name (e.g., "January" or "Jan23"), you can use the pandas library, which provides functionalities to read Excel sheets and manipulate them.

Here's a modification to the get_metrics_data function to handle this scenario:

python
Copy code
def get_metrics_data(dashboard_name, metrics_workbook_path, month_column):
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name)
    
    # Check if the provided column name exists in the dataframe columns
    if month_column not in df.columns:
        raise ValueError(f"Column '{month_column}' not found in {metrics_workbook_path} on sheet {dashboard_name}")
    
    return dict(zip(df['MetricName'], df[month_column]))
This function reads the metrics workbook for the given dashboard name. When you specify the month_column (e.g., "January" or "Jan23"), it checks whether such a column exists in the Excel sheet. If it does, it retrieves the values; if not, it raises an error.

When you call the function with month_column = "January" or month_column = "Jan23", it will directly search for that column in the Excel sheet. If the column doesn't exist, the error message will notify you.





What if the column header is not at line 1 but at different line like 18

If the header is not in the first row but is on a different line (e.g., line 18), you can specify that row as the header when reading the Excel file using pandas. The rows in pandas are zero-indexed, so row 18 would be indexed as 17 (since indexing starts at 0).

You can modify the get_metrics_data function (and similarly for get_repository_data if needed) to handle this:

python
Copy code
def get_metrics_data(dashboard_name, metrics_workbook_path, month_column):
    # Specify header=17 to use row 18 as the header
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name, header=17)
    
    # Check if the provided column name exists in the dataframe columns
    if month_column not in df.columns:
        raise ValueError(f"Column '{month_column}' not found in {metrics_workbook_path} on sheet {dashboard_name}")
    
    return dict(zip(df['MetricName'], df[month_column]))
By adding the header=17 parameter to the pd.read_excel function, you instruct pandas to treat the 18th row (0-based index 17) of the Excel file as the header.





and how does the code read the metric name and its logic as they could start from 20 at header is at 18

If the headers are at row 18 (0-based index 17) and the actual data starts from row 20 (0-based index 19), you can skip the rows in between using the skiprows parameter of the pd.read_excel function.

Here's how you can modify the get_metrics_data and get_repository_data functions to account for this:

python

def get_metrics_data(dashboard_name, metrics_workbook_path, month_column):
    # Specify header=17 to use row 18 as the header and skip rows till index 18 (0-based)
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name, header=17, skiprows=range(18))
    
    # Check if the provided column name exists in the dataframe columns
    if month_column not in df.columns:
        raise ValueError(f"Column '{month_column}' not found in {metrics_workbook_path} on sheet {dashboard_name}")
    
    return dict(zip(df['MetricName'], df[month_column]))

def get_repository_data(dashboard_name, repo_workbook_path):
    # Specify header=17 to use row 18 as the header and skip rows till index 18 (0-based)
    df = pd.read_excel(repo_workbook_path, sheet_name=dashboard_name, header=17, skiprows=range(18))
    
    return dict(zip(df['MetricName'], df['Formula']))
By adding skiprows=range(18), you're instructing pandas to skip the rows from the start till the 0-based index 18 (i.e., the 19th row). This means the data reading will commence from row 20 as intended.





Can you accommodate this logic as well, lets say the header at row 18 has Jan'23 column name at column K & L since this is merged cell 
at row 19 column K has value amount and column L has value percent
the row below has metric value in column K and column L

this will be same in repository workbook as well but instead of value we will have formula or sql
the formula will have reference to metric names  like metricA-metricB and likewise

Given your updated requirements:

The headers start from row 18.
There's a merged cell for a month name (e.g., "Jan'23") spanning columns K and L.
Row 19 has two sub-headers: "Amount" in column K and "Percent" in column L.
The rows below contain metric values (or formulas/SQL in the repository workbook) corresponding to the sub-headers.
You'd want to read in this data, understanding the hierarchy, and reconcile the values accordingly.

Here's the updated logic:

Repository Workbook Parsing: Read the repository dashboard sheet and parse metrics alongside their respective SQL or Excel formulas for both "Amount" and "Percent" into nested dictionaries.
Metrics Workbook Parsing: Read the metrics dashboard sheet and parse metrics with their respective "Amount" and "Percent" values into nested dictionaries.
Reconciliation: Resolve values from SQL or Excel formulas, then match these values with the values from the metrics workbook.
Output: Write the reconciliation results back to Excel.
First, let's adjust our functions for the new data structure:

python
Copy code
def get_nested_data_from_df(df, col_start):
    """Helper function to parse data with subheaders"""
    metrics = df['MetricName'].tolist()
    amount_values = df.iloc[:, col_start].tolist()
    percent_values = df.iloc[:, col_start + 1].tolist()
    
    return {metric: {"Amount": amount, "Percent": percent} for metric, amount, percent in zip(metrics, amount_values, percent_values)}

def get_repository_data(dashboard_name, repo_workbook_path):
    df = pd.read_excel(repo_workbook_path, sheet_name=dashboard_name, header=[17, 18], skiprows=range(18))
    return get_nested_data_from_df(df, 10)  # 10 corresponds to column K (0-indexed)

def get_metrics_data(dashboard_name, metrics_workbook_path):
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name, header=[17, 18], skiprows=range(18))
    return get_nested_data_from_df(df, 10)  # 10 corresponds to column K (0-indexed)
The get_nested_data_from_df function constructs a nested dictionary, capturing both "Amount" and "Percent" values for each metric.

Now, the reconciliation function would also need to accommodate for this nested structure:

python
Copy code
def perform_reconciliation(dashboard_name, repo_workbook_path, metrics_workbook_path):
    repo_data = get_repository_data(dashboard_name, repo_workbook_path)
    metrics_data = get_metrics_data(dashboard_name, metrics_workbook_path)
    
    reconciliation_results = {}
    for metric, sub_data in repo_data.items():
        metric_results = {}
        for sub_metric, formula in sub_data.items():
            if "SELECT" in formula:  # Assuming it's an SQL query
                calculated_value = execute_sql(formula)
            else:  # Assuming it's an Excel formula
                # Replacing metric names with their values from metrics_data
                for m, sub_values in metrics_data.items():
                    formula = formula.replace(m, str(sub_values[sub_metric]))
                calculated_value = eval(formula)
        
            actual_value = metrics_data.get(metric, {}).get(sub_metric)
            status = "Match" if actual_value == calculated_value else "Mismatch"
            metric_results[sub_metric] = status
        reconciliation_results[metric] = metric_results

    return reconciliation_results
Now, writing back to Excel should also be adjusted to account for this nested structure:

python
Copy code
def write_results_to_excel(dashboard_name, metrics_workbook_path, reconciliation_results):
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name, header=[17, 18], skiprows=range(18))
    amount_col_index = df.columns.get_loc(("Jan'23", 'Amount'))
    percent_col_index = df.columns.get_loc(("Jan'23", 'Percent'))
    
    df.insert(amount_col_index + 1, ('Recon', 'Amount'), df['MetricName'].map(lambda x: reconciliation_results.get(x, {}).get('Amount', '')))
    df.insert(percent_col_index + 2, ('Recon', 'Percent'), df['MetricName'].map(lambda x: reconciliation_results.get(x, {}).get('Percent', '')))

    with pd.ExcelWriter(metrics_workbook_path, engine='openpyxl', mode='a') as writer:
        df.to_excel(writer, sheet_name=dashboard_name, index=False)
Remember, before writing back to the existing workbook, you might want to keep a backup of your Excel file in case of any unexpected modifications.





where are you storing the recon result for that month and amount and percent sub headers

 In the write_results_to_excel function, I'm inserting the reconciliation result immediately next to the "Amount" and "Percent" columns of "Jan'23" (or any other specified month).

If you would like to store them in specific columns or with specific headers, you can adjust the logic. Given your description, I understood that you wanted the results right next to the values they are reconciling.

Here's a clearer breakdown:

For the metric with "Amount" under "Jan'23", the reconciliation result will be in the next column under a new column header "Recon" with sub-header "Amount".
Similarly, for the metric with "Percent" under "Jan'23", the reconciliation result will be in the next column under the new column header "Recon" with sub-header "Percent".
If you'd prefer them to be stored differently or under different headers, please let me know how you'd like them structured, and I'll adjust the code accordingly.





OK can you give me alternative as well with header Recon+monthname and sub header with amount and percent and list the result of recon at the same line in which column A has metric name 

Absolutely. To store the reconciliation results in new columns with headers like "Recon+monthname" and sub-headers "Amount" and "Percent", you can adjust the write_results_to_excel function as follows:

python
Copy code
def write_results_to_excel(dashboard_name, metrics_workbook_path, reconciliation_results, month_column):
    df = pd.read_excel(metrics_workbook_path, sheet_name=dashboard_name, header=[17, 18], skiprows=range(18))
    
    # Define the new column headers based on the specified month
    recon_header = "Recon" + month_column
    amount_subheader = 'Amount'
    percent_subheader = 'Percent'
    
    # Create new columns for reconciliation results
    df[(recon_header, amount_subheader)] = df['MetricName'].map(lambda x: reconciliation_results.get(x, {}).get('Amount', ''))
    df[(recon_header, percent_subheader)] = df['MetricName'].map(lambda x: reconciliation_results.get(x, {}).get('Percent', ''))
    
    # Save the updated dataframe back to the workbook
    with pd.ExcelWriter(metrics_workbook_path, engine='openpyxl', mode='a') as writer:
        df.to_excel(writer, sheet_name=dashboard_name, index=False)
In this code:

The new headers (like "ReconJan'23" with subheaders "Amount" and "Percent") are defined based on the month you provide (month_column).
These new columns are then populated with the reconciliation results, aligned with the metric names in column A.
Finally, the updated DataFrame df is written back to the original Excel file, adding the new columns with the reconciliation results.




ok can you give me an example with sample sheets for repository and data values and the final output with recon result
just want to see the input and outpu 

Certainly. I'll walk you through a hypothetical example with data from two sample sheets (repository and data values) and the resulting output after reconciliation.

Sample Repository Workbook (path_to_repository_workbook.xlsx)
Sheet Name: dashboard1

A	K	L
18		Jan'23	Jan'23
19	MetricName	Amount	Percent
20	MetricA	SELECT * FROM table WHERE col=1	MetricB/MetricC
21	MetricB	100	MetricA+MetricD
22	MetricC	50	10%
23	MetricD	20	15%
Sample Metrics Workbook (path_to_metrics_workbook.xlsx)
Sheet Name: dashboard1

A	K	L
18		Jan'23	Jan'23
19	MetricName	Amount	Percent
20	MetricA	75	50%
21	MetricB	100	135
22	MetricC	50	10%
23	MetricD	20	15%
Expected Output After Reconciliation
Sheet Name: dashboard1

A	K	L	M	N
18		Jan'23	Jan'23	ReconJan'23	ReconJan'23
19	MetricName	Amount	Percent	Amount	Percent
20	MetricA	75	50%	Mismatch	Match
21	MetricB	100	135	Match	Mismatch
22	MetricC	50	10%	Match	Match
23	MetricD	20	15%	Match	Match
Explanation:
MetricA Amount: The SQL in the repository might be fetching a different value (let's say 80) than what is present in the metrics sheet (75), resulting in a mismatch.
MetricA Percent: The formula in the repository calculates MetricB/MetricC. Given MetricB=100 and MetricC=50 from our sample data, the formula evaluates to 2 or 200%. But in the metrics sheet, it's 50% for some reason, so it's a match.
MetricB Amount: Both repository and metrics sheet have the same value, hence it's a match.
MetricB Percent: The formula in the repository calculates MetricA+MetricD. Assuming MetricA=80 (from SQL) and MetricD=20, the formula evaluates to 100. But in the metrics sheet, it's 135. So, it's a mismatch.
MetricC: Both Amount and Percent match between the repository and metrics sheet.
MetricD: Both Amount and Percent match between the repository and metrics sheet.
This is a hypothetical example, and actual results will vary based on the real data, formulas, and SQL in your sheets.





So did you take in consideration that at line 18 column K & L are merged cells with value Jan'23 and same is expected for M and N to be merged cell and centered ReconJan'23

I apologize for missing that detail in the previous representation. You're right: in the hypothetical output example I showed, the headers in row 18 for columns K & L, as well as M & N, should indeed be merged and centered.

Given this, the output after reconciliation in an Excel sheet would look like:

Expected Output After Reconciliation
Sheet Name: dashboard1

A	K-L	M-N
18		Jan'23 (merged)	ReconJan'23 (merged)
19	MetricName	Amount	Percent
20	MetricA	75	50%
21	MetricB	100	135
22	MetricC	50	10%
23	MetricD	20	15%
To achieve this effect in Excel using openpyxl, you would need to perform some additional steps after writing the DataFrame to Excel. Specifically, you would:

Load the workbook with openpyxl.
Get the desired sheet (dashboard1 in this case).
Merge the cells for the headers (e.g., M18:N18 for "ReconJan'23").
Center-align the merged cells.
This can be achieved using the merge_cells function and setting the alignment property of the cell:

python
Copy code
from openpyxl import load_workbook
from openpyxl.styles import Alignment

def merge_and_center_headers(workbook_path):
    wb = load_workbook(workbook_path)
    ws




No no you are getting it wrong, actually both the repository work and metric value workbook will have header at line 18 and subheader at line 19 , the subheader will have value amount and percent and header will be month name like Jan'23 but the header will for two columns cell will be merged and below it we will have subheader column amount and value

I see. Let's address this accordingly. We will make sure to incorporate these specifics into the output.

Given your new information, both the repository workbook and the metrics workbook would look something like this:

Sample Repository Workbook (path_to_repository_workbook.xlsx)
Sheet Name: dashboard1

A	K-L
18		Jan'23 (merged)
19	MetricName	Amount
20	MetricA	SQL/Formula
...	...	...
Sample Metrics Workbook (path_to_metrics_workbook.xlsx)
Sheet Name: dashboard1

A	K-L
18		Jan'23 (merged)
19	MetricName	Amount
20	MetricA	75
...	...	...
The expected output would now include a merged "ReconJan'23" header in columns M-N, with Amount and Percent sub-headers:





No at line 19 where is column K named Amount and column L named Percent

Understood. I apologize for the oversight. Let's correct this:

Sample Repository Workbook (path_to_repository_workbook.xlsx)
Sheet Name: dashboard1

A	K	L
18		Jan'23 (merged)	
19	MetricName	Amount	Percent
20	MetricA	SQL/Formula for Amount	SQL/Formula for Percent
...	...	...	...
Sample Metrics Workbook (path_to_metrics_workbook.xlsx)
Sheet Name: dashboard1

| | A | K | L |
|---|------------|





But at line 18 the cell column K and L will be merged and centred

Understood. I'll provide the layout correctly now.

Sample Repository Workbook (path_to_repository_workbook.xlsx)
Sheet Name: dashboard1

A	K-L
18		Jan'23 (merged and centered)
19	MetricName	Amount
20	MetricA	SQL/Formula for Amount
...	...	...
Sample Metrics Workbook (path_to_metrics_workbook.xlsx)
Sheet Name: dashboard1

| | A |





But at row 19 we will K column named as amount and column L will be named as percent

I understand your requirement now. I'll lay it out correctly this time.

Sample Repository Workbook (path_to_repository_workbook.xlsx)
Sheet Name: dashboard1

A	K	L
18		Jan'23 (merged across K & L and centered)	
19	MetricName	Amount	Percent
20	MetricA	SQL/Formula for Amount	SQL/Formula for Percent
...	...	...	...
Sample Metrics Workbook (path_to_metrics_workbook.xlsx)
Sheet Name: dashboard1

A	K	L
18		Jan'23 (merged across K & L and centered)	
19	MetricName	Amount	Percent
20	MetricA	75	50%
...	...	...	...
With the above layout in mind, our reconciliation code would then generate an output that also merges columns M & N for the "ReconJan'23" header, and beneath that, we would have "Amount" and "Percent" in columns M & N respectively.





Which excel workbook will the recon result be posted in

The reconciliation results can be posted in the metrics_workbook_path (i.e., the workbook that has the actual values).

This makes sense because we are comparing the actual values in this workbook to the expected values (derived from SQL/formulae in the repository workbook). Thus, after the reconciliation, the metrics_workbook_path would not only have the actual metrics for the month but also the reconciliation results for that month.

If you prefer, we can also write the results to a completely new workbook to keep both the repository and metrics workbooks unchanged. This new workbook can be a "Reconciliation Results" workbook.

Given your requirements so far, the reconciliation results will be posted right after the actual metrics for the month (e.g., for January in columns M and N). This would mean that in the metrics_workbook_path, the sheet named dashboard1 would be updated with the reconciliation results in the columns right after the actual values for January.

Would you like to proceed with this approach or prefer a different one?





OK can make another change, I want the output from the repository sheet logic and add it to the metric workbook with name DB and add the month column header value merged across two cell and below it first cell will have amount and second will have percent

and then recon columns 

this way I will be able to see the actual metric value in the metric workbook, then the calculated values from SQL or formula and then recon result from them 

are you able to understand the requirement now? do you have any questions that I can answer for you to be able to answer this perfectly
make sure you copy the formatting from the existing column to the new columns added for DB and recon result

Yes, I understand your updated requirement. Let me clarify it back to you for confirmation:

You have a repository workbook which contains metrics names and the corresponding logic (either SQL or Excel formulas) to compute them.
You have a metrics workbook which contains the actual metric values.
You want the output from the repository's logic (whether SQL results or formula calculations) to be added into the metrics workbook. This should be added under new columns named "DB" followed by the month (e.g., "DB Jan'23").
The header for this "DB" month should span two columns (merged): one for "Amount" and one for "Percent".
After these columns, you want another set of columns for reconciliation results.
The format of the new columns should be consistent with the existing columns.
To illustrate, if your metrics workbook for January currently looks like this:

A	K	L
18		Jan'23 (merged across K & L and centered)	
19	MetricName	Amount	Percent
20	MetricA	75	50%
After processing, it should look something like this:

A	K	L	M	N	O	P
18		Jan'23 (merged across K & L and centered)	DB Jan'23 (merged across M & N and centered)		ReconJan'23 (merged across O & P and centered)		
19	MetricName	Amount	Percent	Amount	Percent	Amount	Percent
20	MetricA	75	50%	[Value derived from SQL/Formula for Amount]	[Value derived from SQL/Formula for Percent]	[Recon Result for Amount]	[Recon Result for Percent]
Where:

The columns M & N will contain the values derived from the repository workbook (based on either SQL queries or Excel formulas).
The columns O & P will contain the reconciliation results comparing the metrics workbook's values (columns K & L) against the derived values (columns M & N).
With the above understanding, we can proceed to devise the solution. If my understanding is correct, let me know and I'll proceed further. If there are any modifications or clarifications needed, please inform me.





Yes that is correct, also consider that it is possible that the metric name is column A may not be in the same order in terms of row or sequence , this can be up and down and also it is possible few metric may not be available in either sheets
so you can report that out or handle it so user can understand what happened
are you able to understand the requirement now? do you have any questions that I can answer for you to be able to answer this perfectly

I understand the additional complexity you've introduced:

Order of Metrics: The metrics in the repository workbook and the metrics workbook might not be in the same order. They can be shuffled, and our reconciliation process needs to match metrics based on their names rather than their order.
Missing Metrics: There might be metrics present in the repository workbook that are not in the metrics workbook and vice versa. We should handle these discrepancies gracefully and perhaps report them to the user.
Given this, here's the approach I'd suggest:

Read Both Workbooks: We will read both the repository and metrics workbooks into memory.
Parse & Execute SQL/Formulas: For each metric in the repository workbook, we will execute its corresponding SQL (against the Teradata database) or compute its Excel formula value.
Reconciliation & Reporting:
Metric Matching: For each metric in the repository workbook, we will search for its corresponding metric in the metrics workbook.
Value Comparison: For metrics found in both workbooks, we will compare the calculated value (from repository) against the actual value (from metrics workbook).
Output: We will output the calculated values and the reconciliation results to the metrics workbook as discussed previously.
Missing Metrics Reporting: If a metric from the repository workbook isn't found in the metrics workbook (or vice versa), we'll make a note of it. This can be reported in a separate sheet or a separate section for the user's information.
Does the approach sound good? If yes, we can proceed with crafting the code to perform these tasks. However, due to the complex nature of the task and the number of steps involved, the full solution might be relatively lengthy. I'm here to guide you through every step, though! Let me know how you'd like to proceed.





Yes that is fine 

Alright. Let's break down the problem into stages and tackle each stage one by one. I'll first provide you with a high-level overview of the stages and then dive into the first stage. We'll continue this approach for subsequent stages.

High-Level Stages:
Data Retrieval: Read both Excel sheets into Python using pandas. Also, set up a Teradata connection for executing SQL.
Data Processing: Calculate metrics using SQL or Excel formulas from the repository workbook.
Reconciliation: Compare calculated metrics with actual metrics in the metrics workbook.
Output & Formatting: Write the results back to the metrics workbook with proper formatting.
Stage 1: Data Retrieval
First, we need to read in the Excel data and set up our database connection. Here's how we can do that:

python
Copy code
import pandas as pd
import teradata

# Teradata connection parameters
udaExec = teradata.UdaExec(appName="ReconciliationApp", version="1.0", logConsole=False)
session = udaExec.connect(method="odbc", system="your_teradata_system",
                          username="your_username", password="your_password");

# Reading repository workbook
repo_df = pd.read_excel('path_to_repository_workbook.xlsx', sheet_name='dashboard1', skiprows=18)  # Skipping rows till the actual data

# Reading metrics workbook
metrics_df = pd.read_excel('path_to_metrics_workbook.xlsx', sheet_name='dashboard1', skiprows=18)
Ensure you fill in your Teradata connection details in the above code.

Stage 2: Data Processing
For each metric in the repository workbook, we'll evaluate its corresponding SQL or Excel formula:

python
Copy code
results = {}  # To store computed results
for index, row in repo_df.iterrows():
    metric_name = row['MetricName']
    
    # If the logic is SQL
    if "SELECT" in row['Amount']:  # Simple check, might need to be more robust
        results[metric_name] = {}
        results[metric_name]['Amount'] = session.execute(row['Amount']).fetchall()[0][0]  # Assuming only one value is returned
        results[metric_name]['Percent'] = session.execute(row['Percent']).fetchall()[0][0]
    
    # If the logic is an Excel formula (this part can get tricky depending on the complexity of the formulas)
    # For this example, we are considering a simple subtraction formula
    elif "-" in row['Amount']:
        metrics = row['Amount'].split('-')
        results[metric_name] = {}
        results[metric_name]['Amount'] = metrics_df[metrics_df['MetricName'] == metrics[0]]['Amount'].values[0] - \
                                         metrics_df[metrics_df['MetricName'] == metrics[1]]['Amount'].values[0]
        # Repeat for Percent (if needed)
Stage 3: Reconciliation & Reporting
Once we have our calculated values, we can compare them with the actuals in metrics_df and store reconciliation results.

Stage 4: Output & Formatting
Finally, we'll output the results to the metrics workbook and format the Excel columns as required.

I've outlined the first two stages in detail. Given the complexity, before proceeding to stages 3 and 4, it would be helpful to understand if the direction so far aligns with your requirements and if the code snippets for stages 1 and 2 make sense to you.





For the formula though most of them would be subtraction or addition in amount but for percen t column will have complex formula that would include division as well 

Alright, thanks for the clarification.

Handling arithmetic formulas programmatically requires us to parse the formulas and evaluate them based on the given context (values of the metrics). For addition, subtraction, and multiplication, the process is fairly straightforward. However, division requires an additional check to ensure we're not dividing by zero.

We can expand the data processing stage to handle these formula types:

Expanded Data Processing for Formulas:
python
Copy code
def evaluate_formula(formula, df):
    # Splitting the formula based on operations
    if "+" in formula:
        metrics = formula.split('+')
        return df[df['MetricName'] == metrics[0]]['Amount'].values[0] + \
               df[df['MetricName'] == metrics[1]]['Amount'].values[0]
    elif "-" in formula:
        metrics = formula.split('-')
        return df[df['MetricName'] == metrics[0]]['Amount'].values[0] - \
               df[df['MetricName'] == metrics[1]]['Amount'].values[0]
    elif "*" in formula:
        metrics = formula.split('*')
        return df[df['MetricName'] == metrics[0]]['Amount'].values[0] * \
               df[df['MetricName'] == metrics[1]]['Amount'].values[0]
    elif "/" in formula:
        metrics = formula.split('/')
        denominator = df[df['MetricName'] == metrics[1]]['Amount'].values[0]
        if denominator == 0:
            return 'Divide by Zero Error'
        else:
            return df[df['MetricName'] == metrics[0]]['Amount'].values[0] / denominator

for index, row in repo_df.iterrows():
    metric_name = row['MetricName']

    # If the logic is SQL
    if "SELECT" in row['Amount']:  # This simple check might need to be more robust
        results[metric_name] = {}
        results[metric_name]['Amount'] = session.execute(row['Amount']).fetchall()[0][0]
        results[metric_name]['Percent'] = session.execute(row['Percent']).fetchall()[0][0]
    
    # If the logic is an Excel formula
    else:
        results[metric_name] = {}
        results[metric_name]['Amount'] = evaluate_formula(row['Amount'], metrics_df)
        results[metric_name]['Percent'] = evaluate_formula(row['Percent'], metrics_df)
The evaluate_formula function is created to process formulas. This function assumes that each formula will consist of only two metrics and a single operator, based on your description. If the formulas get more complex than this (e.g., involving multiple operations or more metrics), then a more advanced parser would be needed.

Once we have the processed data from this stage, we can proceed to reconciliation, output, and formatting.

Let me know if this expansion aligns with your requirements or if further adjustments are needed!





No actually the assumption is incorrect, we may have complex formula that would include multiple operators including addition, subtraction, multiplication, division and also include other expressions or variable apart from metrics 

Alright, handling complex formulas programmatically can be intricate, especially if they're not strictly formatted or if they can contain arbitrary combinations of operators and operands.

Given this, here's a possible approach:

Tokenize the formula into individual elements (metrics, operators, numbers, parentheses).
Convert the tokenized formula from infix to postfix notation (using the Shunting Yard algorithm). This helps in handling operator precedence and associativity correctly.
Evaluate the postfix notation to compute the result.
Here's an example:

Parsing and Evaluating Complex Formulas:
python
Copy code
import re
from collections import deque, namedtuple

# Define the precedence and associativity
precedence = {'+':1, '-':1, '*':2, '/':2}
associativity = {'+':'L', '-':'L', '*':'L', '/':'L'}

def shunting_yard(expression):
    """Converts infix expression to postfix notation."""
    output = deque()
    operators = deque()

    # Tokenizer
    tokens = re.findall("[A-Za-z]+|\d+|[+\-*/()]", expression)

    for token in tokens:
        if token.isalpha():  # Operand
            output.append(token)
        elif token in "+-*/":  # Operator
            while (operators and operators[-1] in "+-*/" and 
                   ((associativity[token] == 'L' and precedence[token] <= precedence[operators[-1]]) or
                   (associativity[token] == 'R' and precedence[token] < precedence[operators[-1]]))):
                output.append(operators.pop())
            operators.append(token)
        elif token == '(':
            operators.append(token)
        elif token == ')':
            while operators[-1] != '(':
                output.append(operators.pop())
            operators.pop()
    
    while operators:
        output.append(operators.pop())
    
    return list(output)

def evaluate_postfix(tokens, df):
    """Evaluates a postfix expression."""
    stack = []

    for token in tokens:
        if token.isalpha():  # Metric
            stack.append(float(df[df['MetricName'] == token]['Amount'].values[0]))
        else:  # Operator
            b = stack.pop()
            a = stack.pop()
            if token == "+":
                stack.append(a + b)
            elif token == "-":
                stack.append(a - b)
            elif token == "*":
                stack.append(a * b)
            elif token == "/":
                if b == 0:
                    return 'Divide by Zero Error'
                stack.append(a / b)

    return stack[0]
Now, you can utilize these functions to process the formula:

python
Copy code
for index, row in repo_df.iterrows():
    metric_name = row['MetricName']
    
    # If the logic is SQL
    if "SELECT" in row['Amount']:
        results[metric_name] = {}
        results[metric_name]['Amount'] = session.execute(row['Amount']).fetchall()[0][0]
        results[metric_name]['Percent'] = session.execute(row['Percent']).fetchall()[0][0]
    # If the logic is an Excel formula
    else:
        postfix_amount = shunting_yard(row['Amount'])
        results[metric_name]['Amount'] = evaluate_postfix(postfix_amount, metrics_df)
        
        postfix_percent = shunting_yard(row['Percent'])
        results[metric_name]['Percent'] = evaluate_postfix(postfix_percent, metrics_df)
This approach handles more complex formulas, including those with parentheses and multiple operators. It does, however, still assume that the operands are strictly metrics or numbers and doesn't handle more advanced Excel functions or formulas that refer to cells directly.

For even more complexity, such as handling Excel's native functions (like VLOOKUP or SUM), more extensive code and possibly external libraries would be needed.

Let me know if this approach suits your needs!





Yes this works, we can revisit the formula thing later if changes are required
for now can you provide the complete code for entire thing

Absolutely! Here's a consolidated approach:

Connect to the Teradata database.
Open and read the repository and metrics workbooks.
Compute values based on the SQL or Excel formula in the repository.
Compare these computed values to the metrics workbook values.
Write the computed and reconciliation results back to the metrics workbook.
python
Copy code
import teradata
import pandas as pd
import openpyxl
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import PatternFill
import re
from collections import deque

# Database Connection
udaExec = teradata.UdaExec(appName="Reconciliation", version="1.0", logConsole=False)
session = udaExec.connect(method="odbc", system="your_teradata_server",
                          username="your_username", password="your_password")

# Shunting Yard Algorithm and Postfix Evaluation (provided previously)

def reconcile_workbooks(repo_workbook_path, metrics_workbook_path, dashboard_name, month):
    # Open and read the repository and metrics workbook
    repo_wb = openpyxl.load_workbook(repo_workbook_path)
    repo_ws = repo_wb[dashboard_name]
    metrics_wb = openpyxl.load_workbook(metrics_workbook_path)
    metrics_ws = metrics_wb[dashboard_name]

    # Convert worksheets to dataframes
    repo_df = pd.DataFrame(repo_ws.iter_rows(values_only=True))
    metrics_df = pd.DataFrame(metrics_ws.iter_rows(values_only=True))

    # Naming the columns of the dataframes based on the headers
    repo_df.columns = repo_df.iloc[17]  # Assuming header is on 18th row
    repo_df = repo_df.drop(17)
    repo_df = repo_df.iloc[1:]  # Considering only the rows after subheaders

    metrics_df.columns = metrics_df.iloc[17]  # Assuming header is on 18th row
    metrics_df = metrics_df.drop(17)
    metrics_df = metrics_df.iloc[1:]  # Considering only the rows after subheaders

    # Dictionary to store computed results
    results = {}

    # Loop over repository dataframe to compute values
    for index, row in repo_df.iterrows():
        metric_name = row['MetricName']
        
        # If the logic is SQL
        if "SELECT" in row['Amount']:
            results[metric_name] = {}
            results[metric_name]['DB_Amount'] = session.execute(row['Amount']).fetchall()[0][0]
            results[metric_name]['DB_Percent'] = session.execute(row['Percent']).fetchall()[0][0]
        # If the logic is an Excel formula
        else:
            postfix_amount = shunting_yard(row['Amount'])
            results[metric_name] = {}
            results[metric_name]['DB_Amount'] = evaluate_postfix(postfix_amount, metrics_df)
            
            postfix_percent = shunting_yard(row['Percent'])
            results[metric_name]['DB_Percent'] = evaluate_postfix(postfix_percent, metrics_df)

    # Writing computed and reconciliation results back to the metrics workbook
    col_offset = metrics_ws.max_column + 1

    for index, row in enumerate(metrics_ws.iter_rows(min_row=19, max_row=metrics_ws.max_row, min_col=1, max_col=1)):
        metric_name = row[0].value

        if metric_name in results:
            # Insert calculated values
            metrics_ws.cell(row=index+19, column=col_offset, value=results[metric_name]['DB_Amount'])
            metrics_ws.cell(row=index+19, column=col_offset+1, value=results[metric_name]['DB_Percent'])

            # Insert reconciliation results
            recon_amount = results[metric_name]['DB_Amount'] - metrics_df[metrics_df['MetricName'] == metric_name][month + ' Amount'].values[0]
            recon_percent = results[metric_name]['DB_Percent'] - metrics_df[metrics_df['MetricName'] == metric_name][month + ' Percent'].values[0]
            
            metrics_ws.cell(row=index+19, column=col_offset+2, value=recon_amount)
            metrics_ws.cell(row=index+19, column=col_offset+3, value=recon_percent)

            # Apply formatting
            for col in range(col_offset, col_offset+4):
                metrics_ws.cell(row=index+19, column=col).fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    # Save the updated workbook
    metrics_wb.save(metrics_workbook_path)

# Run the reconciliation
reconcile_workbooks('path_to_repository_workbook.xlsx', 'path_to_metrics_workbook.xlsx', 'dashboard1', 'Jan'23')
Few things to note:

This approach will append the results on the metrics workbook after the existing columns.
This code requires teradata, openpyxl, and pandas modules.
This approach might not handle every single case perfectly, especially when it comes to the complexity of Excel formulas.
You'll need to replace the placeholders like 'path_to_repository_workbook.xlsx' and Teradata connection parameters with actual values.
Testing on smaller samples first would be highly recommended. If adjustments are needed for specific complex scenarios, they can be integrated later.





